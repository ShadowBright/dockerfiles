FROM anapsix/alpine-java

USER root
WORKDIR /tmp

ARG SPARK_VERSION
ARG HADOOP_VERSION
ARG SCALA_VERSION
ARG SBT_VERSION

ENV SPARK_VERSION=${SPARK_VERSION}
ENV HADOOP_VERSION=${HADOOP_VERSION}
ENV SCALA_VERSION=${SCALA_VERSION}
ENV SBT_VERSION=${SBT_VERSION}


ENV SCALA_ARCHIVE=scala-${SCALA_VERSION}.tgz
ENV SBT_ARCHIVE=sbt-${SBT_VERSION}.tgz
ENV SPARK_ARCHIVE=spark-${SPARK_VERSION}-bin-without-hadoop.tgz
ENV HADOOP_ARCHIVE=hadoop-${HADOOP_VERSION}.tar.gz

ENV SPARK_HOME=/opt/spark/
ENV HADOOP_HOME=/opt/hadoop/
ENV HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop/
ENV SCALA_HOME=/opt/scala
ENV SBT_HOME=/opt/sbt


COPY config/repositories /etc/apk/repositories
COPY config/*.rsa.pub /etc/apk/keys/
COPY config/id_rsa.pub /root/.ssh/authorized_keys
COPY config/id_rsa /root/.ssh/id_rsa

RUN mkdir -p /tmp/config && \
    echo export JAVA_HOME=$JAVA_HOME > /tmp/config/env.sh && \
    echo export SPARK_HOME=$SPARK_HOME >> /tmp/config/env.sh && \
    echo export HADOOP_HOME=$HADOOP_HOME >> /tmp/config/env.sh && \
    echo export HADOOP_CONF_DIR=$HADOOP_CONF_DIR >> /tmp/config/env.sh && \
    echo source /tmp/config/init_env.sh >> /tmp/config/.bashrc && \
    echo ". '/opt/spark/sbin/spark-config.sh'" >> /tmp/config/.bashrc && \
    echo ". '/opt/spark/bin/load-spark-env.sh'" >> /tmp/config/.bashrc && \
    echo "export PATH='/usr/bin/scala:/usr/local/sbt/bin:$PATH'" >> /tmp/config/.bashrc && \ 
passwd -d root && \
    chmod 0600 /root/.ssh/id_rsa /root/.ssh/authorized_keys && \
apk add  --virtual build_dependencies \
	alpine-sdk \
	build-base \
    nodejs \
	ca-certificates \
	musl-dev \
	gcc \
	python3-dev \
	make \
	cmake \
	g++ \
	gfortran \
	libpng-dev \
	freetype-dev \
	libxml2-dev \
	libxslt-dev \
    linux-headers \
    zeromq-dev \
    libstdc++ && \
    ln -s /usr/include/locale.h /usr/include/xlocale.h && \
apk add --no-cache \ 
        bash \
	    curl \
	    vim \
		jq \
		wget \
        tar \
        python3\
        py-pip \
        py3-numpy \
        py3-scipy \ 
        py3-pandas \
        py3-matplotlib \
        openssh && \
        pip3 install --no-cache-dir --upgrade setuptools pip && \
echo "Scala Archive: ${SCALA_ARCHIVE}" && \
    mkdir -p "${SCALA_HOME}" && \
    wget -q https://downloads.typesafe.com/scala/${SCALA_VERSION}/${SCALA_ARCHIVE} && \
    tar xzf ${SCALA_ARCHIVE} --strip=1 -C ${SCALA_HOME} && \
    rm ${SCALA_ARCHIVE} && \
    ln -s "${SCALA_HOME}/bin/"* "/usr/bin/" && \
mkdir -p ${SBT_HOME} && \
    wget -q --no-check-certificate https://github.com/sbt/sbt/releases/download/v${SBT_VERSION}/${SBT_ARCHIVE} && \
    tar xzf ${SBT_ARCHIVE} -C ${SBT_HOME} --strip-components=1 && \
    rm ${SBT_ARCHIVE} && \
mkdir -p "${HADOOP_HOME}" && \
    wget -q http://apache.lauf-forum.at/hadoop/common/hadoop-${HADOOP_VERSION}/${HADOOP_ARCHIVE} && \
    tar xzf ${HADOOP_ARCHIVE} --strip=1 -C ${HADOOP_HOME} && \
    rm ${HADOOP_ARCHIVE} && \
mkdir -p /opt/spark && \
    wget -q http://mirror.synyx.de/apache/spark/spark-${SPARK_VERSION}/${SPARK_ARCHIVE} && \
    tar xzf ${SPARK_ARCHIVE} --strip=1 -C /opt/spark && \
    rm ${SPARK_ARCHIVE}

COPY config/hadoop/* ${HADOOP_HOME}/etc/hadoop/
COPY config/spark/* ${SPARK_HOME}/conf/

RUN cat ${HADOOP_HOME}/etc/hadoop/hadoop-env-init.sh ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh >/tmp/hadoop-env.sh && \
    mv /tmp/hadoop-env.sh ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh && \
    rm ${HADOOP_HOME}/etc/hadoop/hadoop-env-init.sh && \
    chmod +x ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh

ENV LANG=C.UTF-8 \
    SHELL=/bin/bash \
    NB_USER=dev \
    NB_UID=1000 \
    NB_GID=100 \
    HOME=/home/dev

RUN adduser -D -u $NB_UID $NB_USER && \
    chmod g+w /etc/passwd /etc/group && \
    apk del --purge -r build_dependencies && \
    cp /tmp/config/.bashrc ~

COPY    config/init_env.sh \
        config/jupyter_notebook_config.py \
        config/entrypoint* \
        config/nbextentions.sh \
        config/requirements.txt /tmp/config/

#pip3 install -r /tmp/config/requirements.txt


#USER dev

WORKDIR /tmp/data
